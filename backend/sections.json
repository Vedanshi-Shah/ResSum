{
    "Abstract": "In many networking systems, Bloom filters are used for high speed set membership tests. They permit a small fraction of false positive answers with very good space e\ufb03ciency. However, they do not permit deletion of items from the set, and previous attempts to extend \u201cstandard\u201d Bloom filters to support deletion all degrade either space or performance. \nWe propose a new data structure called the cuckoo filter that can replace Bloom filters for approximate set membership tests. Cuckoo filters support adding and removing items dynamically while achieving even higher performance than Bloom filters. For applications that store many items and target moderately low false positive rates, cuckoo filters have lower space overhead than space optimized Bloom filters. Our experimental results also show that cuckoo filters outperform previous data structures that extend Bloom filters to support deletions substantially in both time and space. \n",
    "Categories And Subject Descriptors": "E.1 [Data]: Data Structures; E.4 [Data]: Data Compaction and Compression \n",
    "Keywords": "Cuckoo hashing; Bloom filters; compression \n",
    "Introduction": "Many databases, caches, routers, and storage systems use approximate set membership tests to decide if a given item is in a (usually large) set, with some small false positive probability. The most widely used data structure for this test is the Bloom filter , which has been studied extensively due to its memory e\ufb03ciency. Bloom filters have been used to: reduce the space required in probabilistic routing tables ; speed longest prefix matching for IP addresses ; improve network state management and monitoring ; and encode multicast forwarding information in packets , among many other applications . \nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third party components of this work must be honored. For all other uses, contact the Owner/Author. Copyright is held by the Owner/Author(s). \nCoNEXT\u201914, Dec 02 05 2014, Sydney, Australia \nACM 978 1 4503 3279 8/14/12. \nhttp://dx.doi.org/10.1145/2674005.2674994 \nA limitation of standard Bloom filters is that one cannot remove existing items without rebuilding the entire filter (or possibly introducing generally less desirable false negatives). Several approaches extend standard Bloom filters to support deletion, but with significant space or performance overhead. Counting Bloom filters  have been suggested for multiple applications , but they generally use 3\u20134\u00d7 space to retain the same false positive rate as a space optimized Bloom filter. Other variants include d left counting Bloom filters , which are still 1.5\u00d7 larger, and quotient filters , which provide significantly degraded lookup performance to yield comparable space overhead to Bloom filters. \nThis paper shows that supporting deletion in approximate set membership tests need not impose higher overhead in space or performance compared to standard Bloom filters. We propose the cuckoo filter, a practical data structure that provides four major advantages. \nA cuckoo filter is a compact variant of a cuckoo hash table  that stores only fingerprints\u2014a bit string derived from the item using a hash function\u2014for each item inserted, instead of key value pairs. The filter is densely filled with fingerprints , which confers high space e\ufb03ciency. A set membership query for item x simply searches the hash table for the fingerprint of x, and returns true if an identical fingerprint is found. \nWhen constructing a cuckoo filter, its fingerprint size is determined by the target false positive rate \ue00f. Smaller values of \ue00f require longer fingerprints to reject more false queries. Interestingly, while we show that cuckoo filters are practically better than Bloom filters for many real workloads, they are asymptotically worse: the minimum fingerprint size used in the cuckoo filter grows logarithmically with the number of entries in the table . As a consequence, the per item space overhead is higher for larger tables, but this use of extra space confers a lower false positive rate. For practical problems with a few billion items or fewer, \nTable 1: Properties of Bloom filters and variants. Assume standard and counting Bloom filters use k hash functions, and d left counting Bloom filters have d partitions. \na cuckoo filter uses less space while supporting deletion than a non deletable, space optimized Bloom filter when \ue00f < 3%. \nCuckoo filters are substantially di\ufb00erent from regular hash tables because only fingerprints are stored in the filter and the original key and value bits of each item are no longer retrievable. Because full keys are not stored, a cuckoo filter cannot even perform standard cuckoo hashing to insert new items, which involves moving existing keys based on their hash values. This di\ufb00erence means that the standard techniques, analyses, and optimizations that apply to cuckoo hashing do not necessarily carry over to cuckoo filters. \nTechnical contributions made by this paper include \n",
    "Background And Related Work": "\n ### 2.1 Bloom Filters And Variants\nWe compare standard Bloom filters and the variants that include support for deletion or better lookup performance, as summarized in Table 1. These data structures are evaluated empirically in Section 7. Cuckoo filters achieve higher space e\ufb03ciency and performance than these data structures. \nStandard Bloom filters  provide a compact representation of a set of items that supports two operations: Insert and Lookup. A Bloom filter allows a tunable false positive rate \ue00f so that a query returns either \u201cdefinitely not\u201d (with no error), or \u201cprobably yes\u201d (with probability \ue00f of being wrong). The lower \ue00f is, the more space the filter requires. \nA Bloom filter consists of k hash functions and a bit array with all bits initially set to \u201c0\u201d. To insert an item, it hashes this item to k positions in the bit array by k hash functions, and then sets all k bits to \u201c1\u201d. Lookup is processed similarly, except it reads k corresponding bits in the array: if all the bits are set, the query returns true; otherwise it returns false. \nBloom filters do not support deletion. \nBloom filters can be very space e\ufb03cient, but are not optimal . For a false positive rate \ue00f, a space optimized Bloom filter uses k = log2 hash functions. Such a Bloom filter can store each item using 1.44 log2 bits, which depends only on \ue00f rather than the item size or the total number of items. The information theoretic minimum requires log2 bits per item, so a space optimized Bloom filter imposes a 44% space overhead over the information theoretic lower bound. \nThe information theoretic optimum is essentially achievable for a static set by using fingerprints (of length \ue0641/\ue00f\ue065 bits) and a perfect hash table . To e\ufb03ciently handle deletions, we replace a perfect hash function with a well designed cuckoo hash table. \nCounting Bloom filters  extend Bloom filters to allow deletions. A counting Bloom filter uses an array of counters in place of an array of bits. An insert increments the value of k counters instead of simply setting k bits, and a lookup checks if each of the required counters is non zero. The delete operation decrements the values of these k counters. To prevent arithmetic overflow (i.e., incrementing a counter that has the maximum possible value), each counter in the array must be su\ufb03ciently large in order to retain the Bloom filter\u2019s properties. In practice, the counter consists of four or more bits, and a counting Bloom filter therefore requires 4\u00d7 more space than a standard Bloom filter. (One can construct counting Bloom filters to use less space by introducing a secondary hash table structure to manage overflowing counters, at the expense of additional complexity.) \nBlocked Bloom filters  do not support deletion, but provide better spatial locality on lookups. A blocked Bloom filter consists of an array of small Bloom filters, each fitting in one CPU cache line. Each item is stored in only one of these small Bloom filters determined by hash partitioning. As a result, every query causes at most one cache miss to load that Bloom filter, which significantly improves performance. A drawback is that the false positive rate becomes higher because of the imbalanced load across the array of small Bloom filters. \nd left Counting Bloom filters  are similar to the approach we use here. Hash tables using d left hashing  store fingerprints for stored items. These filters delete items by removing their fingerprint. Compared to counting Bloom filters, they reduce the space cost by 50%, usually requiring 1.5 \u2212 2\u00d7 the space compared to a space optimized non deletable Bloom filter. Cuckoo filters achieve better space e\ufb03ciency than d left counting Bloom filters as we show, and have other advantages, including simplicity. \nQuotient filters  are also compact hash tables that store fingerprints to support deletion. Quotient filters uses a technique similar to linear probing to locate a fingerprint, and thus provide better spatial locality. However, they require additional meta data to encode each entry, which requires 10 \u223c 25% more space than a comparable standard Bloom \nFigure 1: Illustration of cuckoo hashing \nfilter. Moreover, all of its operations must decode a sequence of table entries before reaching the target item, and the more the hash table is filled, the longer these sequences become. As a result, its performance drops significantly when the occupancy of the hash table exceeds 75%. \nOther Variants: Other variants have been proposed to improve Bloom filters, either in space and/or performance. Rank Indexed Hashing  builds linear chaining hash tables to store compressed fingerprints. Although similar to and somewhat more space e\ufb03cient than d left counting Bloom filters, updating the internal index that reduces the chaining cost is very expensive, making it less appealing in dynamic settings. Putze et al. proposed two variants of Bloom filters . One is the previously discussed Blocked Bloom filter; the other, called a Golomb Compressed Sequence stores all items\u2019 fingerprints in a sorted list. Its space is near optimal, but the data structure is static and requires non constant lookup time to decode the encoded sequence. It is therefore not evaluated with other filters in this paper. Pagh et al. proposed an asymptotically space optimal data structure  based on Cleary . This data structure, however, is substantially more complex than its alternatives and does not appear amenable to a high performance implementation. In contrast, cuckoo filters are easy to implement. \n\n ### 2.2 Cuckoo Hash Tables\nCuckoo Hashing Basics: A basic cuckoo hash table  consists of an array of buckets where each item has two candidate buckets determined by hash functions h1(x) and h2(x). The lookup procedure checks both buckets to see if either contains this item. Figure 1(a) shows the example of inserting a new item x in to a hash table of 8 buckets, where x can be placed in either buckets 2 or 6. If either of x\u2019s two buckets is empty, the algorithm inserts x to that free bucket and the insertion completes. If neither bucket has space, as is the case in this example, the item selects one of the candidate buckets , kicks out the existing item (in this case \u201ca\u201d) and re inserts this victim item to its own alternate location. In our example, displacing \u201ca\u201d triggers \nanother relocation that kicks existing item \u201cc\u201d from bucket 4 to bucket 1. This procedure may repeat until a vacant bucket is found as illustrated in Figure 1(b), or until a maximum number of displacements is reached . If no vacant bucket is found, this hash table is considered too full to insert. Although cuckoo hashing may execute a sequence of displacements, its amortized insertion time is O. \nCuckoo hashing ensures high space occupancy because it refines earlier item placement decisions when inserting new items. Most practical implementations of cuckoo hashing extend the basic description above by using buckets that hold multiple items, as suggested in . The maximum possible load when using k hash functions and buckets of size b assuming all hash functions are perfectly random has been analyzed . With proper configuration of cuckoo hash table parameters , the table space can be 95% filled with high probability. \nUsing Cuckoo Hashing for Set membership: Recently, standard cuckoo hash tables have been used to provide set membership information in a few applications. To support transactional memory, Sanchez et al. proposed to store the read/write set of memory addresses of each transaction in a cuckoo hash table, and to convert this table to Bloom filters when full . Their design used standard cuckoo hash tables, and thus required much more space than cuckoo filters. Our previous study in building high speed and memory e\ufb03cient key value stores  and software based Ethernet switches  all applied cuckoo hash tables as internal data structures. That work was motivated by and also focused on improving hash table performance by an optimization called partial key cuckoo hashing. However, as we show in this paper, this technique also enabled a new approach to build a Bloom filter replacement which has not been studied before. As a result, this paper also applies partial key cuckoo hashing, but more importantly it o\ufb00ers an in depth analysis of using this technique specifically to serve set membership tests (rather than key value queries) and further compares the performance of cuckoo filters with alternative set membership \ndata structures. \nChallenges in Cuckoo Filter: To make cuckoo filters highly space e\ufb03cient, we use a multi way associative cuckoo hash table to provide high speed lookup and high table occupancy ; to further reduce the hash table size, each item is first hashed into a constant sized fingerprint before inserted into this hash table. The challenge of applying this data structure is to redesign the insert process and carefully configure the hash table to minimize space usage per item: \n",
    "Cuckoo Filter Algorithms": "In this paper, the basic unit of the cuckoo hash tables used for our cuckoo filters is called an entry. Each entry stores one fingerprint. The hash table consists of an array of buckets, where a bucket can have multiple entries. \nThis section describes how cuckoo filters perform Insert, Lookup and Delete operations. Section 3.1 presents partial key cuckoo hashing, a variant of standard cuckoo hashing that enables cuckoo filters to insert new items dynamically. This technique was first introduced in previous work , but there the context was improving the lookup and insert performance of regular cuckoo hash tables where full keys were stored. In contrast, this paper focuses on optimizing and analyzing the space e\ufb03ciency when using partial key cuckoo hashing with only fingerprints, to make cuckoo filters competitive with or even more compact than Bloom filters. \n\n ### 3.1 Insert\nAs previously stated, with standard cuckoo hashing, inserting new items to an existing hash table requires some means of accessing the original existing items in order to determine where to relocate them if needed to make room for the new ones . Cuckoo filters, however, only store fingerprints and therefore there is no way to restore and rehash the original keys to find their alternate locations. To over \nAlgorithm 1: Insert(x) \nf = fingerprint(x); \n1 \ni= hash(x); \n2 \n1 \ni= i\u2295 hash( f ); \nif bucket or bucket has an empty entry then add f to that bucket; \nreturn Done; \n// must relocate existing items; \n1 \n2\ni = randomly pick ior i; \nfor n = 0; n < MaxNumKicks; n++ do \nrandomly select an entry e from bucket[i]; \nswap f and the fingerprint stored in entry e; \ni = i \u2295 hash( f ); \nif bucket[i] has an empty entry then \nadd f to bucket[i]; \nreturn Done; \n// Hashtable is considered full; \nreturn Failure; \ncome this limitation, we utilize a technique called partial key cuckoo hashing to derive an item\u2019s alternate location based on its fingerprint. For an item x, our hashing scheme calculates the indexes of the two candidate buckets as follows: \n \nThe xor operation in Eq.  ensures an important property: h1(x) can also be calculated from h2(x) and the fingerprint using the same formula. In other words, to displace a key originally in bucket i  or h2(x)), we directly calculate its alternate bucket j from the current bucket index i and the fingerprint stored in this bucket by \n \nHence, an insertion only uses information in the table, and never has to retrieve the original item x. \nIn addition, the fingerprint is hashed before it is xor ed with the index of its current bucket to help distribute the items uniformly in the table. If the alternate location were calculated by \u201ci \u2295fingerprint\u201d without hashing the fingerprint, the items kicked out from nearby buckets would land close to each other in the table, if the size of the fingerprint is small compared to the table size. For example, using 8 bit fingerprints the items kicked out from bucket i will be placed to buckets that are at most 256 buckets away from bucket i, because the xor operation would alter the eight low order bits of the bucket index while the higher order bits would not change. Hashing the fingerprints ensures that these items can be relocated to buckets in an entirely di\ufb00erent part of the hash table, hence reducing hash collisions and improving the table utilization. \nUsing partial key cuckoo hashing, cuckoo filters add new items dynamically by the process shown in Algorithm 1. Because these fingerprints can be significantly shorter than the \nsize of h1 or h2, there are two consequences. First, the total number of di\ufb00erent possible choices of  as calculated by Eq.  can be much smaller than using a perfect hash to derive h1 and h2 as in standard cuckoo hashing. This may cause more collisions, and in particular previous analyses for cuckoo hashing (as in ) do not hold. A full analysis of partial key cuckoo hashing remains open (and beyond this paper); in Section 4, we provide a detailed discussion of this issue and consider how to achieve high occupancy for practical workloads. \nSecond, inserting two di\ufb00erent items x and y that have the same fingerprint is fine; it is possible to have the same fingerprint appear multiple times in a bucket. However, cuckoo filters are not suitable for applications that insert the same item more than 2b times (b is the bucket size), because the two buckets for this duplicated item will become overloaded. There are several solutions for such a scenario. First, if the table need not support deletion, then this issue does not arise, because only one copy of each fingerprint must be stored. Second, one could, at some space cost, associate counters with buckets, and increment/decrement them appropriately. Finally, if the original keys are stored somewhere (perhaps in slower external storage), one could consult that record to prevent duplicate insertion entirely, at the cost of slowing down insertion if the table already contains a (false positive) matching entry for the bucket and fingerprint. Similar requirements apply to traditional and d left counting Bloom filters. \n\n ### 3.2 Lookup\nThe lookup process of a cuckoo filter is simple, as shown in Algorithm 2. Given an item x, the algorithm first calculates x\u2019s fingerprint and two candidate buckets according to Eq. . Then these two buckets are read: if any existing fingerprint in either bucket matches, the cuckoo filter returns true, otherwise the filter returns false. Notice that this ensures no false negatives as long as bucket overflow never occurs. \n\n ### 3.3 Delete\nStandard Bloom filters cannot delete, thus removing a single item requires rebuilding the entire filter, while counting Bloom filters require significantly more space. Cuckoo filters are like counting Bloom filters that can delete inserted items by removing corresponding fingerprints from the hash tables on deletion. Other filters with similar deletion processes prove more complex than cuckoo filters. For example, d left counting Bloom filters must use extra counters to prevent the \u201cfalse deletion\u201d problem on fingerprint collision1 , and quotient filters must shift a sequence of fingerprints to fill the \u201cempty\u201d entry after deletion and maintain their \u201cbucket structure\u201d.2 \nThe deletion process of cuckoo filters illustrated in Algorithm 3 is much simpler. It checks both candidate buckets for a given item; if any fingerprint matches in any bucket, one copy of that matched fingerprint is removed from that bucket. \nDeletion does not have to clean the entry after deleting an item. It also avoids the \u201cfalse deletion\u201d problem when two items share one candidate bucket and also have the same fingerprint. For example, if both item x and y reside in bucket i1 and collide on fingerprint f , partial key cuckoo hashing ensures that they both can also reside in bucket i2 because i2 = i1 \u2295 hash( f ). When deleting x, it does not matter if the process removes the copy of f added when inserting x or y. After x is deleted, y is still perceived as a set member because there is a corresponding fingerprint left in either bucket i1 and i2. An important consequence of this is that the false positive behavior of the filter is unchanged after deletion. (In the above example, y being in the table causes false positives for lookups of x, by definition: they share the same bucket and fingerprint.) This is the expected false positive behavior of an approximate set membership data structure, and its probability remains bounded by \ue00f. \nNote that, to delete an item x safely, it must have been previously inserted. Otherwise, deleting a non inserted item might unintentionally remove a real, di\ufb00erent item that happens to share the same fingerprint. This requirement also holds true for all other deletion supporting filters. Short fingerprints su\ufb03ce to approach the optimal load factor achieved by using two fully independent hash functions. Each point is the minimal load factor seen in 10 independent runs. \n",
    "Asymptotic Behavior": "Here we show that using partial key cuckoo hashing to store fingerprints in a cuckoo filter to leads to a lower bound on fingerprint sizes that grows slowly with the filter size. This contrasts with other approaches (such as fingerprints and perfect hashing for a static Bloom filter, previously discussed) where the fingerprint size depended only on the desired false positive probability. While this might seem like a negative, in practice the e\ufb00ect seems unimportant. Empirically, a filter containing up to 4 billion items can e\ufb00ectively fill its hash table with loads of 95% when each bucket holds four fingerprints that are 6 bits or larger. \nThe notation used for our analysis in this and the next section is: \nC average bits per item \nMinimum Fingerprint Size: Our proposed partial key cuckoo hashing derives the alternate bucket of a given item based on its current location and the fingerprint . As a result, the candidate buckets for each item are not independent. For example, assume an item can be placed in bucket i1 or i2. According to Eq. , the number of possible values of i2 is at most 2f when using f  bit fingerprints. Using 1 byte fingerprints, given i1 there are only up to 2f = 256 di\ufb00erent possible values of i2. For a table of m buckets, when 2f < m (or equivalently f < log2 m bits), the choice of i2 is only a subset of all m buckets of the entire hash table. \nIntuitively, if the fingerprints are su\ufb03ciently long, partial key cuckoo hashing could still be a good approximation to standard cuckoo hashing; however, if the hash table is very large and stores relatively short fingerprints, the chance of insertion failures will increase due to hash collisions, which may reduce the table occupancy. This situation may arise when a cuckoo filter targets a large number of items but only a moderately low false positive rate. In the following, we determine analytically a lower bound of the probability of insertion failure. \nLet us first derive the probability that a given set of q items collide in the same two buckets. Assume the first item x has its first bucket i1 and a fingerprint tx. If the other q \u2212 1 items have the same two buckets as this item x, they must 3  have the same fingerprint tx, which occurs with probability 1/2f ; and  have their first bucket either i1 or i1 \u2295 h(tx), which occurs with probability 2/m. Therefore the probability of\ue010 \ue011 such q items sharing the same two buckets is 2/m \u00b7 1/2f q\u22121 . \nNow consider a construction process that inserts n random items to an empty table of m = cn buckets for a constant c and constant bucket size b. Whenever there are q = 2b + 1 items mapped into the same two buckets, the insertion fails. This probability provides a lower bound for failure (and, we believe, dominates the failure probability of this construction process, although we do not prove this and do not need to \ue010in order\ue011 to obtain a lower bound). We might therefore have concerns about the scalability of this approach. As we show next, however, practical applications of cuckoo filters are saved by the b factor in the denominator of the lower bound: as long as we use reasonably sized buckets, the fingerprint size can remain small. \nEmpirical Evaluation: Figure 2 shows the load factor achieved with partial key cuckoo hashing as we vary the fingerprint size f , bucket size b, and number of buckets in the table m. For the experiments, we varied the fingerprint size f from 1 to 20 bits. We run this experiment ten times for filters with m = 215 , 220 , 225 , and 230 buckets, and measured their minimum load over the ten trials. We did not use larger tables due to the memory constraint of our testbed machine. \nAs shown in Figure 2, across di\ufb00erent configurations, filters with b = 4 could be filled to 95% occupancy, and with b = 8 could be filled to 98%, with su\ufb03ciently long fingerprints. After that, increasing the fingerprint size has almost no return in term of improving the load factor (but of course it reduces the false positive rate). As suggested by the theory, the minimum f required to achieve close to optimal occupancy increases as the filter becomes larger. Also, comparing Figure 2(a) and Figure 2(b), the minimum f for high occupancy is reduced when bucket size b becomes larger, as the theory also suggests. Overall, short fingerprints appear to su\ufb03ce for realistic sized sets of items. \nInsights: The lower bound of fingerprint size derived in Eq. , together with the empirical results shown in Figure 2, give important insights into the cuckoo filter. For a Bloom filter, achieving \ue00f = 1% requires roughly 10 bits per item, regardless of whether one thousand, one million, or billion items are stored. In contrast, cuckoo filters require longer fingerprints to retain the same high space e\ufb03ciency of their hash tables, but lower false positive rates are achieved accordingly. We find that, for practical purposes, it can be treated as a reasonable sized constant for implementation. Figure 2 shows that for cuckoo filters targeting a few billion items, 6 bit fingerprints are su\ufb03cient to ensure very high utilization of the hash table. \nAs a heuristic, partial key cuckoo hashing is very e\ufb03cient, as we show further in Section 7. Several theoretical questions regarding this technique, however, remain open for future study, including proving bounds on the cost of inserting a new item and studying how much independence is required of the hash functions. \n",
    "Space Optimizations": "The basic algorithms for cuckoo filter operations Insert, Lookup, and Delete presented in Section 3 are independent of the hash table configuration (e.g., how many entries each bucket has). However, choosing the right parameters for cuckoo filters can significantly a\ufb00ect space e\ufb03ciency. This section focuses on optimizing the space e\ufb03ciency of cuckoo filters, through parameter choices and additional mechanisms. \nSpace e\ufb03ciency is measured by the average number of bits to represent each item in a full filter, derived by the table size divided by the total number of items that a filter can e\ufb00ectively store. Recall that, although each entry of the hash table stores one fingerprint, not all entries are occupied: there must be some slack in the table for the cuckoo filter or there will be failures when inserting items. The following section studies how to (approximately) minimize Eq.  given a target false positive rate \ue00f by choosing the optimal bucket size b. \n\n ### 5.1 Optimal Bucket Size\nKeeping a cuckoo filter\u2019s total size constant but changing the bucket size leads to two consequences: \nTable 2: Space and lookup cost of Bloom filters and three cuckoo filters. \nFigure 3: Amortized space cost per item vs. measured false positive rate, with di\ufb00erent bucket size b = 2, 4, 8. Each point is the average of 10 runs \nthe worst case in which they are; this gives us a reasonably accurate estimate for a table that is 95% full.) In each entry, the probability that a query is matched against one stored fingerprint and returns a false positive successful match is at most 1/2f . After making 2b such comparisons, the upper bound of the total probability of a false fingerprint hit is \n \nwhich is proportional to the bucket size b. To retain the target false positive rate \ue00f, the filter ensures 2b/2f \u2264 \ue00f, thus the minimal fingerprint size required is approximately: \nbits. The average space cost C by Eq.05, Eq.  shows cuckoo filters are asymptotically better (by a constant factor) than Bloom filters, which require 1.44 log2  bits or more for each item. \nOptimal bucket size b To compare the space e\ufb03ciency by using di\ufb00erent bucket sizes b, we run experiments that first construct cuckoo hash tables by partial key cuckoo hashing with di\ufb00erent fingerprint sizes, and measure the amortized space cost per item and their achieved false positive rates. As \nshown in Figure 3, the space optimal bucket size depends on the target false positive rate \ue00f: when \ue00f > 0.002, having two entries per bucket yields slightly better results than using four entries per bucket; when \ue00f decreases to 0.00001 < \ue00f \u2264 0.002, four entries per bucket minimizes space. \nIn summary, we choose  cuckoo filter (i.e., each item has two candidate buckets and each bucket has up to four fingerprints) as the default configuration, because it achieves the best or close to best space e\ufb03ciency for the false positive rates that most practical applications  may be interested in. In the following, we present a technique that further saves space for cuckoo filters with b = 4 by encoding each bucket. \n\n ### 5.2 Semi Sorting Buckets To Save Space\nThis subsection describes a technique for cuckoo filters with b = 4 entries per bucket that saves one bit per item. This optimization is based on the fact that the order of fingerprints within a bucket does not a\ufb00ect the query results. Based on this observation, we can compress each bucket by first sorting its fingerprints and then encoding the sequence of sorted fingerprints. This scheme is similar to the \u201csemi sorting buckets\u201d optimization used in . \nThe following example illustrates how the compression saves space. Assume each bucket contains b = 4 fingerprints and each fingerprint is f = 4 bits (more general cases will be discussed later). An uncompressed bucket occupies 4\u00d74 = 16 bits. However, if we sort all four 4 bit fingerprints stored in this bucket (empty entries are treated as storing fingerprints of value \u201c0\u201d), there are only 3876 possible outcomes in total (the number of unique combinations with replacement). If we precompute and store these 3876 possible bucket values in an extra table, and replace the original bucket with an index into this precomputed table, then each original bucket can be represented by a 12 bit index  rather than 16 bits, saving 1 bit per fingerprint.4 \nNote that this permutation based encoding (i.e., indexing all possible outcomes) requires extra encoding/decoding tables and indirections on each lookup. Therefore, to achieve high lookup performance it is important to make the encod ing/decoding table small enough to fit in cache. As a result, our \u201csemi sorting\u201d optimization only apply this technique for tables with buckets of four entries. Also, when fingerprints are larger than four bits, only the four most significant bits of \nFigure 4: False positive rate vs. space cost per element. For low false positive rates (< 3%), cuckoo filters require fewer bits per element than the space optimized Bloom filters. The load factors to calculate space cost of cuckoo filters are obtained empirically. \neach fingerprint are encoded; the remainder are stored directly and separately. \n",
    "Comparison With Bloom Filter": "We compare Bloom filters and cuckoo filters using the metrics shown in Table 2 and several additional factors. \nSpace E\ufb03ciency: Table 2 compares space optimized Bloom filters and cuckoo filters with and without semi sorting. Figure 4 further shows the bits to represent one item required by these schemes, when \ue00f varies from 0.001% to 10%. The information theoretical bound requires log2 bits for each item, and an optimal Bloom filter uses 1.44 log2 bits per item, for a 44% overhead. Thus cuckoo filters with semi sorting are more space e\ufb03cient than Bloom filters when \ue00f < 3%. \nNumber of Memory Accesses: For Bloom filters with k hash functions, a positive query must read k bits from the bit array. For space optimized Bloom filters that require k = log2, as \ue00f gets smaller, positive queries must probe more bits and are likely to incur more cache line misses when reading each bit. For example, k = 2 when \ue00f = 25%, but k is 7 when \ue00f = 1%, which is more commonly seen in practice. A negative query to a space optimized Bloom filter reads two bits on average before it returns, because half of the bits are set. Any query to a cuckoo filter, positive or negative, always reads a fixed number of buckets, resulting in (at most) two cache line misses. \nValue Association: Cuckoo filters can be extended to also return an associated value (stored external to the filter) for each matched fingerprint. This property of cuckoo filters provides an approximate table lookup mechanism, which returns 1 + \ue00f values on average for each existing item (as it can match more than one fingerprint due to false positive hits) and on average \ue00f values for each non existing item. Standard Bloom filters do not o\ufb00er this functionality. Although variants like Bloomier filters can generalize Bloom filters to represent \nfunctions, these structures are more complex and require more space than cuckoo filters . \nMaximum Capacity: Cuckoo filters have a load threshold. After reaching the maximum feasible load factor, insertions are non trivially and increasingly likely to fail, so the hash table must expand in order to store more items. In contrast, one can keep inserting new items into a Bloom filter, at the cost of an increasing false positive rate. To maintain the same target false positive rate, the Bloom filter must also expand. \nLimited Duplicates: If the cuckoo filter supports deletion, it must store multiple copies of the same item. Inserting the same item kb + 1 times will cause the insertion to fail. This is similar to counting Bloom filters where duplicate insertion causes counter overflow. However, there is no e\ufb00ect from inserting identical items multiple times into Bloom filters, or a non deletable cuckoo filter. \n",
    "Evaluation": "Our implementation5 consists of approximately 500 lines of C++ code for standard cuckoo filters, and 500 lines for the support of the \u201csemi sorting\u201d optimization presented in Section 5.2. In the following, we denote a basic cuckoo filter as \u201cCF\u201d, and a cuckoo filter with semi sorting as \u201css CF\u201d. In addition to cuckoo filters, we implemented four other filters for comparison: \nTable 3: Space e\ufb03ciency and construction speed. All filters are 192MB. Entries in bold are the best among the row. \nhave the same number of buckets; each bucket has four entries. \nWe emphasize that the standard and blocked Bloom filters do not support deletion, and thus are compared as a baseline. \nExperiment Setup: All the items to insert are pre generated 64 bit integers from random number generators. We did not eliminate duplicated items because the probability of duplicates is very small. \nOn each query, all filters first generate a 64 bit hash of the item using CityHash . Each filter then partitions these 64 bits in this hash as it needed. For example, Bloom filters treat the high 32 bits and low 32 bits as the first two independent hashes respectively, then use these two 32 bit values to calculate the other k \u2212 2 hashes. The time to compute the 64 bit hash  is included in our measurement. All experiments use a machine with two Intel Xeon processors  and 32 GB DRAM. \nMetrics: To fully understand how di\ufb00erent filters realize the trade o\ufb00s in function, space and performance, we compare above filters by the following metrics: \n\n ### 7.1 Achieved False Positive Rate\nWe first evaluate the space e\ufb03ciency and false positive rates. In each run, all filters are configured to have the same size . Bloom filters are configured to use nine hash functions, which minimizes the false positive rate with thirteen bits per item. For cuckoo filters, their hash tables have m = 225 buckets each consisting of four 12 bit entries. The d left counting Bloom filter have the same number of hash table entries, but divided into d = 4 partitions. Quotient filter also has 227 entries where each entry stores 3 bit meta data and a 9 bit remainder. \nEach filter is initially empty and items are placed until \neither the filter sees an insert failure (for CF, and dl CBF), or it has reached the target capacity limit (for BF, blk BF, and QF). The construction rate and false positive rate of di\ufb00erent filters are shown in Table 3. \nAmong all filters, the ss CF achieves the lowest false positive rate. Using about the same amount of space , enabling semi sorting can encode one more bit into each item\u2019s fingerprint and thus halve the false positive rate from 0.19% to 0.09%, On the other hand, semi sorting requires encoding and decoding when accessing each bucket, and thus the construction rate is reduced from 5.00 to 3.13 million items per second. \nThe BF and blk BF both use 13.00 bits per item with k = 9 hash functions, but the false positive rate of the blocked filter is 2\u00d7 higher than the BF and 4\u00d7 higher than the best CF. This di\ufb00erence is because the blk BF assigns each item to a single block by hashing and an imbalanced mapping will create \u201chot\u201d blocks that serve more items than average and \u201ccold\u201d blocks that are less utilized. Unfortunately, such an imbalanced assignment happens across blocks even when strong hash functions are used , which increases the overall false positive rate. On the other hand, by operating in a single cache line for any query, the blk BF achieves the highest construction rate. \nThe QF spends more bits per item than BFs and CFs, achieving the second best false positive rate. Due to the cost of encoding and decoding each bucket, its construction rate is the lowest. \nFinally, the dl CBF sees insert failures and stops construction when the entire table is about 78% full, thus storing many fewer items. Its achieved false positive rate is much worse than the other filters because each lookup must check 16 entries, hence having a higher chance of hash collisions. \n\n ### 7.2 Lookup Performance\nDi\ufb00erent Workloads We next benchmark lookup performance after the filters are filled. This section compares the lookup throughput and latency with varying workloads. The workload is characterized by the fraction of positive queries (i.e., items in the table) and negative queries (i.e., items not in the table), which can a\ufb00ect the lookup speed. We vary the fraction p of positive queries in the input workload from 0% (all queries are negative) to 100% (all queries are positive). \nThe benchmark result of lookup throughput is shown in Figure 5. Each filter occupies 192 MB, much larger than the L3 cache  in our testbed. \nFigure 5: Lookup performance when a filter achieves its capacity. Each point is the average of 10 runs. \nThe blk BF performs well when all queries are negative, because each lookup can return immediately after fetching the first \u201c0\u201d bit. However, its performance declines when more queries are positive, because it must read additional bits as part of the lookup. The throughput of BF changes similarly when p increases, but is about 4 MOPS slower. This is because the BF may incur multiple cache misses to complete one lookup whereas the blocked version can always operate in one cache line and have at most one cache miss for each lookup. \nIn contrast, a CF always fetches two buckets7 , and thus achieves the same high performance when queries are 100% positive and 100% negative. The performance drops slightly when p = 50% because the CPU\u2019s branch prediction is least accurate . With semi sorting, the throughput of CF shows a similar trend when the fraction of positive queries increases, but it is lower due to the extra decoding overhead when reading each bucket. In return for the performance penalty, the semi sorting version reduces the false positive rate by a factor of two compared to the standard cuckoo filter. However, the ss CF still outperforms BFs when more than 25% of queries are positive. \nThe QF performs the worst among all filters. When a QF is 90% filled, a lookup must search a long chain of table entries and decode each of them for the target item. \nThe dl CBF outperforms the ss CF, but 30% slower than a BF. It also keeps about the same performance when serving all negative queries and all positive queries, because only a constant number of entries are searched on each lookup. \nDi\ufb00erent Occupancy In this experiment, we measure the lookup throughput when the these filters are filled at di\ufb00erent levels of occupancy. Figure 6 shows the average instantaneous lookup throughput when all queries are negative (i.e., for non existing items) and all queries are \nFigure 7: Insert throughput at di\ufb00erent occupancy. Insert random keys in a large universe until each data structure achieves its designed capacity. Each point is the average of 10 runs. \npositive (i.e., for existing items). \nThe throughput of CF and ss CF is mostly stable across different load factor levels on both negative and positive queries. This is because the total number of entries to read and compare remains constant even as more items are inserted. \nIn contrast, the throughput of QF decreases substantially when more loaded. This filter searches an increasingly long chain of entries for the target item as the load factor grows. \nBoth BF and blk BF behave di\ufb00erently when serving negative and positive queries. For positive queries, they must always check in total k bits, no matter how many items have been inserted, thus providing constant lookup throughput; while for negative queries, when the filter is less loaded, fewer bits are set and a lookup can return earlier when seeing a \u201c0\u201d. \nThe dl CBF behaves di\ufb00erently from the BF. When all lookups are negative, it ensures constant throughput like the CF, because a total of 16 entries from four buckets must be searched, no matter how many items this filter contains. For positive queries, if there are fewer items inserted, the lookup may return earlier before all four buckets are checked; however, this di\ufb00erence becomes negligible after the dl CBF is about 20% filled. \n\n ### 7.3 Insert Performance\nThe overall construction speed, measured based on the total number of items a full filter contains and the total time to insert these items, is shown in Table 3. We also study the instantaneous insert throughput across the construction process. Namely, we measure the insert throughput of di\ufb00erent filters when they are at levels of load factors, as shown in Figure 7. \nIn contrast to the lookup throughput shown in Figure 6, both types of CF have decreasing insert throughput when they are more filled (though their overall construction speed is still high), while both BF and blk BF ensure almost constant insert throughput. The CF may have to move a sequence of existing keys recursively before successfully inserting a new item, and this process becomes more expensive when the load factor grows higher. In contrast, both Bloom filters always \nFigure 6: Lookup throughput (MOPS) at di\ufb00erent occupancy. Each point is the average of 10 runs. \nFigure 8: Delete until empty throughput (MOPS) at different occupancy. Each point is the average of 10 runs. \nset k bits, regardless of the load factor. \nThe QF also has decreasing insert throughput, because it must shift a sequence of items before inserting a new item, and this sequence grows longer when the table is more filled. \nThe dl CBF keeps constant throughput. For each insert, it must only find an empty entry in up to four buckets; if such an entry can not be found, the insert stops without relocating existing items as in cuckoo hashing. This is also why its maximum load factor is no more than 80%. \n\n ### 7.4 Delete Performance\nFigure 8 compares the delete performance among filters supporting deletion. The experiment deletes keys from an initially full filter until it is empty. The CF achieves the highest throughput. Both CF and ss CF provide stable performance through the entire process. The dl CBF performs the second best among all filters. The QF is the slowest when close to full, but becomes faster than ss CF when close to empty. \nEvaluation Summary: The CF ensures high and stable lookup performance for di\ufb00erent workloads and at di\ufb00erent levels of occupancy. Its insert throughput declines as the filter is more filled, but the overall construction rate is still \nfaster than other filters except the blk BF. Enabling semi sorting makes cuckoo filters more space e\ufb03cient than space optimized BFs. It also makes lookups, inserts, and deletes slower, but still faster than conventional BFs. \n",
    "Conclusion": "Cuckoo filters are a new data structure for approximate set membership queries that can be used for many networking problems formerly solved using Bloom filters. Cuckoo filters improve upon Bloom filters in three ways:  support for deleting items dynamically;  better lookup performance; and  better space e\ufb03ciency for applications requiring low false positive rates (\ue00f < 3%). A cuckoo filter stores the fingerprints of a set of items based on cuckoo hashing, thus achieving high space occupancy. As a further key contribution, we have applied partial key cuckoo hashing, which makes cuckoo filters significantly more e\ufb03cient by allowing relocation based on only the stored fingerprint. Our configuration exploration suggests that the cuckoo filter, which uses buckets of size 4, will perform well for a wide range of applications, although appealingly cuckoo filter parameters can be easily varied for application dependent tuning. \nWhile we expect that further extensions and optimizations to cuckoo filters are possible and will further provide impetus for their use, the data structure as described is a fast and e\ufb03cient building block already well suited to the practical demands of networking and distributed systems. \n",
    "Acknowledgments": "This work is supported by funding from Intel via the Intel Science and Technology Center for Cloud Computing (ISTC CC), and National Science Foundation under awards CCF 0964474, CNS 1040801, CCF 1320231, CNS 1228598, and IIS 0964473. We gratefully acknowledge the CoNEXT reviewers for their feedback and suggestions; Rasmus Pagh for discussions on analyzing space usage of cuckoo filters that led to many of the insights about the asymptotic behavior; Hyeontaek Lim for code optimizations; and Iulian Moraru, Andrew Pavlo, Peter Steenkiste for comments on writing. \n",
    "References": ""
}